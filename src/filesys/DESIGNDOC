             +-------------------------+
             |         CS 212          |
             | PROJECT 4: FILE SYSTEMS |
             |     DESIGN DOCUMENT     |
             +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Abraham Yosef <ayosef@stanford.edu>
Anthony Mensah <admensah@stanford.edu>
Gordon Martinez-Piedra <martigp@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

Two pieces of important synchronization were not addressed the Buffer Cache
section. The first was the use of the condition variable. The second was the
global `get_new_lock`.

Threads do no have a lock when they read / write to a cached block, they only 
acquire corresponding entry the lock before to update the entry's metadata to 
say they are reading and writing (`accessed`, `total_refs`, `write_refs`) and 
after to update the entry's metadata to indicate they have finished doing so. 
This allows concurrent read and write to all cached blocks or including
concurrent R/W to a single cached block. Because of this, the write back thread,
that needs to ensure no threads are writing to the block while writing it back
waits for a condition variable that is signalled once `write_refs` is
decremented to 0 before it acquires the entry's lock to gain exclusive access
of the entry.

With just fine grained locking for buffer cache entries, a race condition exists
when two threads are trying to access a block that isn't in the cache. Once
they both iterate through the table and realize the entry isn't there they need
to synchronize to ensure that only one of them loads the block in. To do this
the `get_new_lock` is acquired when acquiring and entry and loading the desired
block into it. Therefore, once the `get_new_lock` is acquired, a second check
is done to verify that the sector the thread is attempted to load in wasn't 
loaded between the time it first checked the buffer cache and when it acquired
the lock. The reason we do a second check instead of using a global lock during
the first check, is to allow concurrent access to block sectors that are
currently cached. 

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

             INDEXED AND EXTENSIBLE FILES
             ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Changes to inode.c:

    /* All used for the multilevel index inode. */
    #define NUM_DIRECT_POINTERS 122
    /* Index of singly indirect block in inode. */
    #define SINGLE_INDIRECT_INDEX NUM_DIRECT_POINTERS
    /* Index of doublly indirect block in inode. */
    #define DOUBLE_INDIRECT_INDEX NUM_DIRECT_POINTERS + 1
    /* Number of block pointers in an inode. */
    #define NUM_BLOCK_POINTERS 124
    /* Number of block pointers in an indirect block. */
    #define POINTERS_PER_BLOCK  128
    /* Max number of indicies needed to get data block in inode. */
    #define MAX_INDICIES 3
    /* Max number of bytes a file can be. */
    #define MAX_FILE_BYTES (NUM_DIRECT_POINTERS + NUM_BLOCK_POINTERS + \
                            NUM_BLOCK_POINTERS * NUM_BLOCK_POINTERS)   \
                            * BLOCK_SECTOR_SIZE                        \

    /* Synchronization for `open_inodes` list */
    static struct lock open_inodes_lock;

    Changed `struct inode_disk` to:

        struct inode_disk
        {
            off_t length;                               /* File size in bytes.*/
            block_sector_t blocks [NUM_BLOCK_POINTERS]; /* Block Pointers. */
            bool is_file;                               /* Whether inode
                                                           represents file or
                                                           dir.*/
            unsigned magic;                             /* Magic number. */
        };

    Added members `struct inode` to:

        int write_cnt;                      /* Number of writers. */
        
        struct condition no_writers;        /* Condition variable to set
                                               deny_write_cnt when no more
                                               users. */
    
        struct lock deny_write_lock;        /* Sync for deny_write_cnt. */
        struct lock extension_lock;         /* Sync for inode extension. Makes
                                               extension of inode and updating
                                               inode length atomic. */
------------------------------------------------------------------------------
Changes to free_map.c

    static struct lock free_map_lock;    /* Lock to synchronize access to free
                                            map. */

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

128 indexes per block.
122 direct indexes in inode
1 singly indirect index in inode
1 doublly indirect index in inode

Direct = 1 block pointer => 512B
Singly indirect = 128 direct => 64kB
Doubly indirecty = 128 singly direct => 8MB

In total
(122 * 512) + (128 * 512) + (128 * 128 * 512)
This comes out to:
8.12 MB
8317 KB
8516608 B


---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

Interleaved writing and reading when not extending a file is achieved with
the use of the `SHARE` type accesses. These two accesses release the lock on
the relevant cached block in the buffer cache once they have updated the 
relevant metadata, meaning that mutiple processes can read and write to a block
in a file if doing so before the EOF.

Writing in the `inode_write_at` funciton is done in block increments, where
you write a block at a time. Before accessing getting the relevant sector number
of the block to write to, a synchronously checks what the length of the file is 
by using the `EXCL` enum when acquiring the lock. This flag ensures that 
upon return the process has exclusive access to the cached inode block and 
therefore the store of the current length of the data the inode represents
The `EXCL`flag is described in detail in Section C. If the process sees that
the current write would extend the file it acquires the extension lock.

The `extension_lock` synchronizes these actions for processes extending a file:
    1) acquiring the block (and alllocating if neeeded).
    2) Writing to the block.
    3) Updating the length field in the inode. 

Moreover there are no race conditions between two processes trying to extend
a file at the same time. Note that writing is done in block increments and so
the extension lock not held for the entire duration of a write, but for each
individual block write.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

The first measure to avoid this race was to store the length of an inode on the
disk sector on which the inode is stored. This ensures that we can use the fine
grain synchronization of the buffer cache entries to our advantage for this 
race. It means that access to the length of the inode is synchronized.

The second measure was making it the case that a writer that is extending a file
only updates the length of the file once it has completed the block write that
extends the file.

Specifically, it uses the `EXCL` flag when it accesses to update the length.
This flag ensures uses the condition variable `no_refs` to wait there are no
users of the cached inode sector, gains exclusive control and then updates the
length. A second flag passed to the buffer cache once finished, to marks the
page as dirty so it is written back to disk by the write back funciton.

Correspondingly, a process is only able to read up until EOF, which is
represented by the `length` of the inode. This means that Process A has to wait
until the write is complete and B has updated the length member before it can
read what B has written beyond the EOF before B's write.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

Starvation is avoided by the use of the `shared_waiters`, `excl_waiters`,
`no_refs`, `shared_refs` and `excl_done` members of each cache entry. After
acquiring the lock on a buffer cache, processes check if there are any waiters
of a different kind on the cached block. For example when process A wants
EXCL access, it first checks if there are any `shared_waiters`. If there are
A goes to sleep on the condition variable `no_refs` and waits to be signalled
by the process of those shared waiters that decrements `shared_refs` to 0.
Similarly, those gaining shared access check to see whether there are any
`excl_waiters` and if so goes to sleep on the `excl_done` condition variable.

Once and exclusive process finishes with its access, it broadcasts to the
`excl_waiters` condition variable to wake up any processes attempted to gain
shared access. Similarly, the shared process to last stop using a cached_block
signals the `no_refs` condition variable to wake up the next process attempting
to gain exclusive access.

This queuing mechanism ensures fairness by making sure that processes that
are made to wait / sleep and put in a queued order. Each single exclusive access
is interleaved with every shared access that comes after it gains the lock.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

We implemented a multilevel index with as many direct blocks as possible
with room for one indirect and one doubly indirect (witin the 512B block limit).
The doubly indirect was necessary in order to meet allow for 8MB files
criteria. We chose to maximise the number of direct in order increase the 
threshold of file size, beyond which indirect indexing (and therefore more disk
accesses) are necessary. Most "small files" are smaller than 64KB (approximately
the number of bytes that can be indexed directly in our design). The reason we
chose to have only one indirect index was to balance the trade off of maximising
direct, but also wanting to allow for files of medium size ~64 - 128KB to not
require double indexing (and so more disk access). Hence we include one singly
indirect index.

This was supported by similar design choices made by Unix in their filesystem.

                SUBDIRECTORIES
                ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Directories start off with two entries: '.' and '..' */
#define NUM_INITIAL_DIRENTS 2

/* Macros to improve readbility of functions that have logic conditioned
   on whether file or directory. */
#define IS_FILE true
#define IS_DIR false

    /* The type of an file descriptor, either file, directory or reserved. */
    enum fd_type
        {
            FILE,
            DIR,
            RESERVED,
        };

    /* A pointer to either a file or a directory in the file
    descriptor table.  */
    union fdt_ptr
        {
            struct file *file;
            struct dir *dir;
        };

    /* Entry in the file descriptor table. */
    struct fdt_entry
        {
            union fdt_ptr fp;          /* Directory or File Pointer. */
            enum fd_type type;         /* File Descriptor Table Entry Type. */
        };

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

                 BUFFER CACHE
                 ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Size of buffer cache. */
#define CACHE_SIZE 64
/* Maximum number of iterations over buffer cache for eviction. */                            
#define MAX_CLOCK_LOOPS 2
/* Number of ticks between periodic buffer cache write back.
   Equivalent of 10 seconds. */
#define WRITE_BACK_PERIOD TIMER_FREQ * 10

Added new data structure to represent cache entries.

/* An entry in the buffer cache. */
struct cache_entry
    {
        block_sector_t sector;          /* Block sector represented. */
        int shared_refs;                /* Number of shared references. */
        int shared_waiters;             /* Number of processes waiting for
                                           shared access. */
        int excl_waiters;               /* Number processes waiting for
                                           exclusive access. */
        bool accessed;                  /* Accessed bit for eviction. */
        bool dirty;                     /* Dirty bit for eviction and write
                                           back. */
        bool allocated;                 /* Whether cache_entry has been 
                                           allocated. */
        struct lock lock;               /* Lock to synchronize access to cache
                                           entry metadata. */
        struct condition excl_done;     /* Signal shared waiters who attempted
                                           to access cache after an attempt
                                           to gain exclusive access. */
        struct condition no_refs;       /* Signal exclusive waiter to access
                                           cache after shared use. */
        uint8_t *data;                  /* Actual cached sector. */
    };

New data structure to hold information necessary for read ahead thread to
to read ahead and all necessary I/O associated with it instead of the process
adding to the read ahead queue.

    struct r_ahead_data
        {
            block_sector_t inode_sector;        /* Inode sector. */
            off_t ofs;                          /* Offest within Inode of
                                                read ahead data. */
        };

New data structure to represent entry in the read ahead queue:

    struct r_ahead_entry 
        {
            struct r_ahead_data data;               /* Data to do read ahead. */
            struct list_elem list_elem;             /* Read ahead queue elem. */
        };



Enumerated type to represent the different type of cache use. These enums
are used as an argument to cache_get_entry and ensure that when the function
returns the cache_entry has had the appropriate synchronization applied.

    enum cache_use_type
        {
            EXCL,                         /* Exclusive access. */
            SHARE,                        /* Shared exclusive access. */
            R_AHEAD,                      /* Read ahead. */
        };

Static members of cache.c:

    /* Synchronization when loading block sector into cache for race between
    two processes that both can't find block sector in cache. Prevents double
    loading of that block sector. */

    static struct lock get_new_lock;

    /* Block Cache Begin. */
    static struct cache_entry *cache_begin;
    /* Buffer cache end (for iteration). */
    static struct cache_entry *cache_end;
    /* Clock hand for eviction. */
    static struct cache_entry *clock_hand;
    /* Count of cache entries in use. */
    static int cached_count;
    /* Filesystem device for filesys R/W. */
    extern struct block *fs_device;

    /* Queue of sectors to be read ahead by the read ahead thread. */
    struct list read_ahead_queue;
    /* Sync for read_ahead_queue. */
    struct lock read_ahead_lock;
    /* CV to put read ahead thread to sleep if no sectors to read. */
    struct condition read_ahead_cv;


---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We implement a version of the clock algorithm for our cache replacement
algorithm. Accordingly each entry has an `accessed` flag that is set whenever
its corresponding cached block sector is read from or written to.

The buffer cache is statically allocated at boot (i.e. doesn't change size once
allocated), so each entry has an `allocated` flag to make sure only allocated
entries are considered for eviction. Similarly the `total_refs` member tracks
whether the cache block is being used. If `total_refs` is greater than 0, an
entry is not considered for eviction.

The clock hand is initialized (at boot) to point to the first entry in the
buffer cache. During eviction, the algorithm acquires the lock for the entry 
that the clock hand is pointing to. If the entry is not allocated the lock on 
the entry is released and the clock hand proceeds. Next the eviction algorithm
waits on the total_refs of the cache entry to go to zero using a condition 
variable. This is to ensure fairness of the clock algorithm (i.e. no pinning 
as with VM).

Next the accessed flag is checked. If it is set, it is set to 0 and the clock 
hand procceeds (once the lock is dropped). Otherwise that cache entry is chosen
for eviction and the clock hand is incremented by one for the next eviciton.

If an entry isnt' selected after two loops of the circular buffer cache, then
the eviction algorithm stops and NULL is returned. Two loops chosen to handle
the case that all accessed flags were set of entries that were allocated and so
a second loop of the buffer cache was required to select an entry for eviction.

>> C3: Describe your implementation of write-behind.

When the buffer cache is created and initialized at boot, the `write-back`
thread is created. The `write-back` thread wakes up every 10 seconds and
writes back each entry in the buffer cache that is dirty. Whether a cache is
dirty is tracked by the `dirty` flag in each cache entry. This flag is set
when the cached block sector is written to and initialized to false. The thread
is given default priority (31).

To write behind, the `write-back` thread iterates across the buffer cache.
For each entry it acquires the entry using the `EXCL` flag. This means once 
the thread acquires the lock on the entry it waits on the condition variable 
`no_refs` to ensure that no threads are currently writing to the entry. Once no
more threads are writing to cached block sector, it gains exclusive access, 
writes the cached block back to disk if it is dirty and then moves on to the 
next cache entry (after dropping the current one's lock). Once it has iterated 
across the whole buffer cache it goes back to sleep for 10 seconds.

On shutdown, the main thread writes back each allocated cache entry and frees
all associated memory with each entry. After iterating through the list is also
frees the entire buffer cache.

>> C4: Describe your implementation of read-ahead.


Whenever a block sector is read in from disk into the buffer cache, the next
block sector is read into the buffer cache asynchronously by adding the block
to the `read_ahead_queue`. This queue is read from by the `read-ahead` thread
that is created on boot when the buffer cache is initialized.

Adding to and removing from the read ahead queue is synchonized through
the condition variable `read_queue_cv` and the lock `read_queue_lock`. Adding
to the read ahead queue requires acquiring the `read_queue_lock`. Similarly,
whenever an entry is added the condition variable is signaled to tell the
`read-ahead` thread that it has an entry to put on the list. Similarly, the read
ahead thread loops on the read_ahead queue, acquiring the lock and waiting on 
the condition variable if there are no entries in the queue.

Note to ensure fairness the read ahead thread is given default priority (31) 
and it only reasons one entry at a time to prevent starvation of processes
trying to add to the queue. Additionally, the read ahead queue checks the free
map to ensure the block sector is actually allocated before reading it in.


---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

Each cache entry has a lock that protects its metadata. Before a process
accesses a cache entry, it first has to acquire the lock on that cache_entry.
When process A reads or writes to a buffer cache block, it first acquires the
cache entry lock and increments `shared_refs` (assuming it is not extending).
It then releases the lock and begins reading / writing to the cached block.
Once it has completed reading / writing, it acquires the lock again and 
decrements `shared_refs`. This means whenever a process is actively 
reading / writing data in a buffer cache block that the `shared_refs`is
non-zero. There is also the case when a process has exclusive access to a block,
in this case it simply holds the lock for the duration of its access.

Correspondingly, during eviction, when process B is deciding whether to evict
a block from the cache it does so using the `EXCL` enum. This is the same
synchronization mechanism used by the write behind thread. B first acquires the
cache lock. From this point on, a processes that tries to gain access to the 
cache entry will have to wait until B is done. Next B inspects `share_refs` 
member to check whether any processes are currently accessing the block. If 
this member is  non-zero, it blocks on the condition variable `no_refs`. The 
process that  decrements `shared_refs` to zero signals the process B when 
finished writing / reading. This means that blocks that are being actively 
written to or read from i.e. `shared_refs` is non zero, are never evicted. The 
evicting process is not starved via the mechanism described in A5.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

Similar to above, when performing the eviction selection algorithm, the
process acquires exclusive access to the cache entry by using the `EXCL`
`enum cache_use_type` in the call to `cache_get_entry`. This flag ensures that
no other processes are currently using the cached block. This is achieved by
the mechanism described above of acquring the lock, meaning no other processes
have exclusive access and then waiting on the condition variable until all 
current users are finished using it. If selected, the lock is still held and it 
is guaranteed that no other block is still accessing the block (since 
`shared_refs` must be 0 to wake up from the condition variable `no_refs`. 
This lock is only released once  eviction is completed. Since a process must 
acquire the cache entry' lock in order to access the block, if another process 
attempt to access the block during up until the moment that the eventually 
evicted block is selected to determine whether eligible for eviction it must 
wait until eviction is complete (when the entry's lock is released).

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Buffer caching:
    
Iterating through a large file would significanly benefit from buffer
caching as it would allow the inode (and relevant singly and doubly indirect
blocks) to remain in the buffer cache. This would significantly speed up
indexing by reducing the number of disk accesses required to index into the
correct block.

Another example is performing several reads of the same small section of a file,
for example a small file that can fit in the buffer cache. This prevents
the number of N - 1 disk accesses, where N is the number of accesses.
Multiple processes reading the same segement of a file concurrently / in 
parallel would also hugely benefit from this.


Read Ahead:

Similar to the above case, reading an entire file from start to finish. Process
A that wants to read the file doesn't have to block on I/O with read ahead.
Instead, disk retrieval is offloaded to the read ahead thread, meaing CPU 
ultilization can be increased as reading from disk and CPU execution can happen
in parallel.

Write Behind:

Any process that is frequently writing to the same section of a file will
benefit from write back. Updates to the file can happen in memory and not
require reading from disk and writing through each time a single write is
performed. Only when the block hasn't been written to for a long time is
the block written back, writing several updates to disk with one disk write.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?
