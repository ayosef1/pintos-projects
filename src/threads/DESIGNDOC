            +--------------------+
            |        CS 212      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

FirstName LastName <email@domain.example>
FirstName LastName <email@domain.example>
Gordon Martinez-Piedra <martigp@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct` or
>> `struct` member, global or static variable, `typedef`, or
>> enumeration.  Identify the purpose of each in 25 words or less.

The `sleeping_list` is a list of threads put to sleep by
`timer_sleep ()` ordered by wakeup time. A book keeping mechanism
to wake threads in order of `wake_time`:

    `static struct list sleeping_list`

Added to struct thread:

    /* Members to allow thread sleep functionality*/
    int64_t wake_time;                  /* Time at which thread should wake
                                           after being put to sleep. */
    struct semaphore *wake_sema;        /* Used to put thread to sleep and 
                                           to wake it up. */
    struct list_elem sleep_elem;        /* List element for sleeping threads
                                           list. */




---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

The thread’s wake time is set and its `wake_sema`, that blocks until it
is woken up, is initialised. Interrupts are blocked to add to thread
to ordered the `sleeping_list`. Finally, the thread blocks
on its `wake_sema` semaphore (with `sema_down`).

After the global `tick` is incremented in the interrupt handler, the
handler makes a call to `thread_wake_sleeping ()` passing the value 
of `tick`. This function wakes up all threads in the `sleeping_list`
for whom it is past their `wake_time`. Waking a thread involves 
removal from the `sleeping_list`, adding to the `ready_list` and
unblocking each of threads’ `wake_sema`(with `sema_up`).

Once the thread is made the running thread, the thread continues in
`timer_sleep ()` and nullifies its `wake_time` and `wake_sema`.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The primary step was the ordering of the  `sleeping_list` by `wake_time`.
The expense of having to do ordered insertion occurs in `timer_sleep ()`
and allows for less time to be spent when evicting (in the handler).
Therefore in the best case we only look at the first element (when the
first element's `wake_time` is greater than `tick`. In the worst case
we have to iterate through the entire list. Without ordering, time in
the handler would be this worst case scenario every time.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

The area where race conditions could appear are in modifying
the `sleeping_list`. Interrupts are disabled while this action
is performed. Synchronisation on these data structures from synch.h
is not used for reasons outlined in A5. There is no concern with
racing on blocking as each thread block on its own `wake_sema`,
and the semaphore is only accessed by the thread that it is a
member of.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Since the interrupt handler also accesses the `sleeping_list` and
`next_wakeup_time` data structure, race conditions exist here between
the handler and `timer_sleep ()` calls. Since the interrupt handler
can’t use synchronisation primitives (as it can’t sleep) any access
/ modification to these two must occur when interrupts are disabled
in order to avoid race conditions.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Initially we considered a global semaphore that would act as the
`sleeping_list` so that we didn’t need to have a semaphore for 
every thread. However, this would mean altering the semaphore 
implementation to have eviction by `wake_time` which would interfere
with the semaphore existing functionality of general purpose blocking.
Having the `wake_sema` for each thread and the global `sleeping_list`
allowed ordered waking up by `wake_time`.

We also considered having a global `next_wake_time` to optimise on
reducing time spent in the interrupt handler. However accessing the
first element of the `sleeping_list` and checking its wake time was
chosen because it didn't require a new global and is not much more
expensive.

Finally, in order to minimize space taken up by thread fields we had
a pointer to a semaphore instead of a semaphore as a member of
`struct thread`. After this we decided instead of creating and 
initialising the semaphore at thread creation (which would require
heap allocation with `malloc`), to simplify the design the a new 
semaphore was stack allocated each call to `timer_sleep ()`. While
more inefficient, the design allows for scoping for the `wake_sema`
(as it should only be used during `timer_sleep ()` and doesn't require
freeing the space allocated for a wake semaphore. 

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

New member added to `struct lock`:
    struct list_elem locks_held_elem; /* List element for the per thread
                                         locks_held by thread */

New members added to `struct thread`:                  
    int original_priority;       /* User set thread priority. */     
    struct list locks_held;      /* List of locks held by this thread. */
    struct lock *waiting_lock;   /* Lock thread is waiting for (if any)*/

`locks_held` : A list of the locks the thread holds, used when
recalculating `priority` when a lock is released (by looking at the
waiters in the locks’ semaphores.
`waiting_lock`: A pointer to the lock that the thread is waiting on.
Used for recursive priority donation to access and donate donate to
nested threads (holders of the `waiting_lock`).
`original_priority`: The priority set by the user, used as factor to
determine the threads `priority` in tandem with donation.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Link to diagram: https://web.stanford.edu/~martigp/%20pri_donation.png

No new data structure is used to track priority donation. Instead it 
is tracked through bookkeeping inside the `thread` and `lock` structs
(with the new methods listed above). In particular, when a thread fails
to acquire a lock, it will updated its `waiting_lock` and then recursively
donate by using the `waiting_lock` to access a lock and the holder member
of the lock to access next nested thread. It then updates the `priority`
if the donated is larger. When successfully acquiring a lock, the lock is
added to the `locks_held` list. Priority isn’t changed because the highest
priority runnable thread is always scheduled and when a lock is released
the highest priority waiting thread is evicted from the waiters list.

Finally when releasing a lock, a thread updates priority by looking through
its `locks_held` list and investigating each of that locks waiting list.


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

When a synchronization primitive is signaled to wake up a thread, it will
find the waiting thread with the highest priority and evict that thread.

Specifically, for a semaphore the thread whose priority is largest 
in the semaphore will be evicted from the `waiters` list. This is done
by using the `list_max` function and passing in a comparison function
that compares the priorities of threads. Note that doing ordered insertion
will not work because priorities can change while threads are in the wait list.
A lock is just a semaphore under the hood, making a call to `sema_up` to wake
up the next thread and so uses the same mechanism.

For a condition variable, we take a similar approach by using the `list_max`
function and custom comparison function to find the highest priority thread in
its `waiters` list and waking that element.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

In lock acquire, the thread attempts to decrement the semaphore but doesn't
block on failure. If this call is unsuccessful, then the thread performs a
recursive priority donation starting on the thread that currently holds the
lock the current thread is trying to acquire. This is done by looking at the
holder of a lock and donating the current thread’s `priority` if the current
thread’s priority is greater. Recursion occurs when a lock holder is also 
waiting for a lock (indicated by the `waiting_lock` member of the thread).
Nested donation is limited to donating down 8 levels.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When the lock is released, the lock is removed from the running thread’s
list of locks held. The running thread then recalculates its priority
by looking at all the locks it holds and taking the maximum
priority from all threads directly waiting for those specific locks and its
user set priority.
 
Finally  the thread with the highest priority is removed from the the lock's
semaphore's list of waiting threds and added to the of the ready queue.
If the removed highest priority thread has a higher priority than the running
thread’s new priority, then the current running thread will yield or if in an
interrupt context, will yield on return, to enable the newly evicted, higher
priority thread to run.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

In thread_set_priority, the thread needs to determine whether to change
its effective priority (`priority`) on top of its base priority
(`original_priority`). This is done by comparing the current `priority`
and `new_priority` that is being set. However a thread’s priority is
changeable by other threads (for example when priority donation occurs)
and so synchronisation is required to prevent this data race on thread's 
`priority`. In this case we cannot use a lock to avoid this race.
In particular in our example, this would require acquiring a lock when
performing priority donation to change the `priority` of a thread.
However, since priority donation occurs inside a call to `lock_acquire` on
the priority of the thread, this would mean recursively calling 
`lock_acquire` inside `lock_acquire` raises a number of issues. Moreover,
we must disable interrupts to allow for synchronisation.

One such issue is causing deadlock. Consider the case that before thread_1,
acquires the lock for its priority in a call to `thread_set_priority`, the
context switches to another thread that is donating its priority to 
thread_1 because thread_1 holds a lock. If the context switched back to
thread_1 before thread_2 had released the lock for thread_1’s priority,
there would be deadlock.

Another data race is in accessing the ready queue to check whether the
thread should yield. This also requires disabling interrupts for
synchronisation as threads add and remove each other to this list
when interrupts are disabled such as in `thread_unblock` calls.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

The way in which we managed priority donation was with a single
priority value and an inductive approach to its value. The base case
is at the beginning of the program, when the `priority` of a thread is
set. The inductive step involves each moment when priorities can change
`priority` (donating priority when a thread fails to acquire a lock,
recalculating a thread’s `priority` when the thread releases a lock,
and recalculating its `priority` when the thread calls 
`thread_set_priority`). If we ensure at the end of each of these actions
all priorities are set correctly, we can assume they are correct at
the beginning of each of these actions, making the work done simple.
In an original implementation, when locks were released we would
recursively go up the donation tree / lock waiting graph to find
what the new priority is. With the inductive approach, all that was
needed was to find the max of the priorities of threads in the wait
list for the locks it holds (i.e. no recursion).

This implentation had the strength of simplicity, where all priority
operations only involved reading/writing to a single value per thread.
In parituclar it optimized on making priority donation a fast process
where at each level a single priority value is checked (and updated if
necessary). Similarly, it produced the simple check as to whether the
priority was less than the donated priority to decided whether to 
continue donating. Since we have our inductive structure, we know that
if the priority is greater than or equal to the donation priority, all
threads below in the lock waiting graph will also have this a priority
greater than or equal to the donation priority (or at least those in
the 8 level range).

A different approach where a list of priorities and associated locks
are maintained would produce a more complicated process of having to
traverse a list each time a thread wants to find its priority. For 
example donation would involve traversing this priority list and 
finding the priority associated with the relevant lock and performing
updates. However, this approach is better than ours for priority
recalculation when a lock is released (simply finding the max of the 
priority list (instead of looking at every thread).

Note that implementation also means that prioirty donation is
inconsistently capped either 8 or 9 levels. Since when updating a priority
when a lock is release you could obtain a new priority from a thread
that has a donated priority that has come down 8 levels already.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Added globals and typedef for fixed point arithmetic and conversion
between fixed point and ints:

    /* We use a signed 32-bit integer to create
      a P.Q fixed point number representation. */

    typedef int32_t fixed_point;

    /* Default to using 17.14 representation. 
      Note: p + q must equal 31 */
    #define P 17         /* The P value in P.Q reprentatoin */
    #define Q 14         /* The Q value in P.Q representation */

    /* Conversion constant fixed point and integers */
    #define F (1 << Q)

    /* Weight factors used in updating the load average */
    #define LOAD_WEIGHT (59 * F / 60)   /* Weight of load average term */
    #define READY_WEIGHT (1 * F / 60)   /* Weight of number ready threads
                                           term */

    /* Constant factor used to get fixed points as integers*/
    #define PRINT_FP_CONST 100

Added two globals to thread.c to keep track of load average and number
ready threads.

    /* Estimate of average number of threads ready to run over 
      the past minute. */
    static fixed_point load_avg;

    /* The number of threads in the ready_list */
    static int num_ready;

New members added to `struct thread`:

      fixed_point recent_cpu_time;        /* Exponentially weighted moving 
                                           average of recent CPU time. */
                                      


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59        A
 4      4   0   0  62  61  59        A
 8      8   0   0  61  61  59        B
12      8   4   0  61  60  59        A        
16     12   4   0  60  60  59        B
20     12   8   0  60  59  59        A
24     16   8   0  59  59  59        C
28     16   8   4  59  59  58        B
32     16  12   4  59  58  58        A
36     20  12   4  57  58  58        C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

The ambiguity was whether `recent_cpu_time` should be change before or
after the priority changed on ticks where both ought should be change
(i.e. every 4 ticks). Similarly, whether the priority should be calculated
before or after a thread yields. In this case we decided to change the
`recent_cpu_time` prior to changing priority on ticks where the two coincided.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

In our implementation all of the scheduling occurs inside an interrupt
context. We maintain a single queue instead of a different queue for
each priority. Functionally this behaves the same way as it.

// Heavy calculations are gonna harm performance. 

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

In our implementation all of the scheduling occurs inside an interrupt
context. We maintain a single queue instead of a different queue for
each priority. Functionally this behaves the same way as the list_max
function's selection criteria is priority and then proximity to the
head as a tie breaker. Since adding to the queue is done using push back
this means that round robin ordering is respected for threads with the
same priority.

The advantage of this design is once again in its simplicity. Maintaining
more than one queue means that threads will be changing 
an operation that is expensive. In our case, we simply need to push update
thread priorities correctly and push threads to the back of the queue once
their tiem slice is done and the list_max function will abstract the
remaining scheduling functionality.

The disadvantage is in its inefficiency. Having a single queue means that 

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

Fundamentally, we made abstractions to make the code a lot more readable
both for us as we were developing, but also for those reading and developing
the code in the future.

The first abstraction we made was to create a new typedef of fixed_point.
While just an int32_t under the hood, this allowed good code readability
to know when a value is being interpreted as an integer of fixed point.

Simiarly we developed a number of functions that specified the types of
each operand and the operation (by function name and parameter types) as
well as the return type (by the return type). This abstracted away the
difficulty of having to do each of the complicated conversions and
fixed point operations each time and clear what each line of code was doing.

Finally, we the macro F as it is a value used repeatedly in fixed point
calculations. Q and P provide another layer of abstraction (even though P)
is not used strictly, to allow those reading the code to understand where
the fixed point representation choice. 

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
